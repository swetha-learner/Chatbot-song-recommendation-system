{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5240720,"sourceType":"datasetVersion","datasetId":3049211},{"sourceId":10827346,"sourceType":"datasetVersion","datasetId":6723155}],"dockerImageVersionId":30445,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/swethaudayakumar3990/chatbot-recommendation-system-chat-with-boo?scriptVersionId=224620670\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install anvil-uplink","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:54:50.444114Z","iopub.execute_input":"2023-05-10T05:54:50.44457Z","iopub.status.idle":"2023-05-10T05:55:02.675986Z","shell.execute_reply.started":"2023-05-10T05:54:50.44453Z","shell.execute_reply":"2023-05-10T05:55:02.673579Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import anvil.server","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:02.679645Z","iopub.execute_input":"2023-05-10T05:55:02.680277Z","iopub.status.idle":"2023-05-10T05:55:02.720859Z","shell.execute_reply.started":"2023-05-10T05:55:02.680213Z","shell.execute_reply":"2023-05-10T05:55:02.718942Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uplink_key = \"server_LID76LQSQGVLZXI7YQTOAWTE-KNHRWSQVOPHD2KU5\"\nanvil.server.connect(uplink_key)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:02.723038Z","iopub.execute_input":"2023-05-10T05:55:02.723637Z","iopub.status.idle":"2023-05-10T05:55:02.865485Z","shell.execute_reply.started":"2023-05-10T05:55:02.72358Z","shell.execute_reply":"2023-05-10T05:55:02.864439Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:02.867189Z","iopub.execute_input":"2023-05-10T05:55:02.867546Z","iopub.status.idle":"2023-05-10T05:55:12.768189Z","shell.execute_reply.started":"2023-05-10T05:55:02.867513Z","shell.execute_reply":"2023-05-10T05:55:12.766767Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nimport json\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:12.771639Z","iopub.execute_input":"2023-05-10T05:55:12.773227Z","iopub.status.idle":"2023-05-10T05:55:14.233101Z","shell.execute_reply.started":"2023-05-10T05:55:12.773166Z","shell.execute_reply":"2023-05-10T05:55:14.231693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nintents_file = open('/kaggle/input/content-checks/intents_check_1.json').read()\nintents = json.loads(intents_file)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:14.235086Z","iopub.execute_input":"2023-05-10T05:55:14.236006Z","iopub.status.idle":"2023-05-10T05:55:14.251796Z","shell.execute_reply.started":"2023-05-10T05:55:14.235954Z","shell.execute_reply":"2023-05-10T05:55:14.250557Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nwords=[]\nclasses = []\ndocuments = []\nfor intent in intents['intents']:\n    # print(intent)\n    for pattern in intent['patterns']:\n        #tokenize each word\n        word = nltk.word_tokenize(pattern)\n        words.extend(word)\n        #add documents in the corpus\n        documents.append((word, intent['tag']))\n        # add to our classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\nprint(documents)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:14.254042Z","iopub.execute_input":"2023-05-10T05:55:14.254804Z","iopub.status.idle":"2023-05-10T05:55:14.294674Z","shell.execute_reply.started":"2023-05-10T05:55:14.254752Z","shell.execute_reply":"2023-05-10T05:55:14.293106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gc import collect;\nfrom IPython.display import clear_output;\nimport nltk;\ndler = nltk.downloader.Downloader();\ndler._update_index();\nnltk.download('omw-1.4');\n\nclear_output();\nfor i in range(3): collect(i);","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:14.296729Z","iopub.execute_input":"2023-05-10T05:55:14.29747Z","iopub.status.idle":"2023-05-10T05:55:14.820014Z","shell.execute_reply.started":"2023-05-10T05:55:14.297416Z","shell.execute_reply":"2023-05-10T05:55:14.818655Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:14.821498Z","iopub.execute_input":"2023-05-10T05:55:14.823084Z","iopub.status.idle":"2023-05-10T05:55:14.893199Z","shell.execute_reply.started":"2023-05-10T05:55:14.823012Z","shell.execute_reply":"2023-05-10T05:55:14.891839Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet \n\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nnltk.download('wordnet2022')\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:14.89502Z","iopub.execute_input":"2023-05-10T05:55:14.895901Z","iopub.status.idle":"2023-05-10T05:55:16.592135Z","shell.execute_reply.started":"2023-05-10T05:55:14.895846Z","shell.execute_reply":"2023-05-10T05:55:16.590051Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ls /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:16.595178Z","iopub.execute_input":"2023-05-10T05:55:16.595749Z","iopub.status.idle":"2023-05-10T05:55:17.650195Z","shell.execute_reply.started":"2023-05-10T05:55:16.595691Z","shell.execute_reply":"2023-05-10T05:55:17.648366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training = []\n# create empty array for the output\noutput_empty = [0] * len(classes)\n# training set, bag of words for every sentence\nfor doc in documents:\n    # initializing bag of words\n    bag = []\n    # list of tokenized words for the pattern\n    word_patterns = doc[0]\n    # lemmatize each word - create base word, in attempt to represent related words\n    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n    # create the bag of words array with 1, if word is found in current pattern\n    for word in words:\n        bag.append(1) if word in word_patterns else bag.append(0)\n    # output is a '0' for each tag and '1' for current tag (for each pattern)\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n    training.append([bag, output_row])\n# shuffle the features and make numpy array\nrandom.shuffle(training)\ntraining = np.array(training)\n# create training and testing lists. X - patterns, Y - intents\ntrain_x = list(training[:,0])\ntrain_y = list(training[:,1])\nprint(\"Training data is created\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:17.653321Z","iopub.execute_input":"2023-05-10T05:55:17.653774Z","iopub.status.idle":"2023-05-10T05:55:19.979968Z","shell.execute_reply.started":"2023-05-10T05:55:17.653732Z","shell.execute_reply":"2023-05-10T05:55:19.978349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(len(train_x[0]),), activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(train_y[0]), activation = 'softmax'))\n# Compiling model. SGD with Nesterov accelerated gradient gives good results for this model\nsgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True) #optimizer \nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n#Training and saving the model \nhist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\nmodel.save('chatbot_model.h5', hist)\npickle.dump(hist,open('model.pkl','wb'))\nprint(\"model is created\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:19.982442Z","iopub.execute_input":"2023-05-10T05:55:19.982883Z","iopub.status.idle":"2023-05-10T05:55:32.100349Z","shell.execute_reply.started":"2023-05-10T05:55:19.982844Z","shell.execute_reply":"2023-05-10T05:55:32.098728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:32.106386Z","iopub.execute_input":"2023-05-10T05:55:32.106828Z","iopub.status.idle":"2023-05-10T05:55:32.136849Z","shell.execute_reply.started":"2023-05-10T05:55:32.106786Z","shell.execute_reply":"2023-05-10T05:55:32.135728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='simple.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:32.137992Z","iopub.execute_input":"2023-05-10T05:55:32.138371Z","iopub.status.idle":"2023-05-10T05:55:32.394881Z","shell.execute_reply.started":"2023-05-10T05:55:32.138336Z","shell.execute_reply":"2023-05-10T05:55:32.393388Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nimport pickle\nimport numpy as np\nfrom keras.models import load_model\nmodel = load_model('chatbot_model.h5')\nimport json\nimport random\nintents = json.loads(open('/kaggle/input/content-checks/intents_check_1.json').read())\nwords = pickle.load(open('/kaggle/input/contents/words.pkl','rb'))\nclasses = pickle.load(open('/kaggle/input/contents/classes.pkl','rb'))\ndef clean_up_sentence(sentence):\n    # tokenize the pattern - splitting words into array\n    sentence_words = nltk.word_tokenize(sentence)\n    # stemming every word - reducing to base form\n    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n    return sentence_words\n# return bag of words array: 0 or 1 for words that exist in sentence\ndef bag_of_words(sentence, words, show_details=True):\n    # tokenizing patterns\n    sentence_words = clean_up_sentence(sentence)\n    # bag of words - vocabulary matrix\n    bag = [0]*len(words)  \n    for s in sentence_words:\n        for i,word in enumerate(words):\n            if word == s: \n                # assign 1 if current word is in the vocabulary position\n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % word)\n    return(np.array(bag))\ndef predict_class(sentence):\n    # filter below  threshold predictions\n    p = bag_of_words(sentence, words,show_details=False)\n    res = model.predict(np.array([p]))[0]\n    ERROR_THRESHOLD = 0.25\n    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n    # sorting strength probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n    return return_list\ndef getResponse(ints, intents_json):\n    tag = ints[0]['intent']\n    list_of_intents = intents_json['intents']\n    for i in list_of_intents:\n        if(i['tag']== tag):\n            result = random.choice(i['responses'])\n            break\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:32.397561Z","iopub.execute_input":"2023-05-10T05:55:32.39813Z","iopub.status.idle":"2023-05-10T05:55:32.536163Z","shell.execute_reply.started":"2023-05-10T05:55:32.398071Z","shell.execute_reply":"2023-05-10T05:55:32.534266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\nimport random\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nimport json\nimport pickle\n\nwords=[]\nclasses = []\ndocuments = []\nignore_letters = ['!', '?', ',', '.']\nintents_file = open('/kaggle/input/content-checks/intents_check_1.json').read()\nintents = json.loads(intents_file)\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        #tokenize each word\n        word = nltk.word_tokenize(pattern)\n        words.extend(word)\n        #add documents in the corpus\n        documents.append((word, intent['tag']))\n        # add to our classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\nprint(documents)\n# lemmaztize and lower each word and remove duplicates\nwords = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\nwords = sorted(list(set(words)))\n# sort classes\nclasses = sorted(list(set(classes)))\n# documents = combination between patterns and intents\nprint (len(documents), \"documents\")\n# classes = intents\nprint (len(classes), \"classes\", classes)\n# words = all words, vocabulary\nprint (len(words), \"unique lemmatized words\", words)\n\npickle.dump(words,open('words.pkl','wb'))\npickle.dump(classes,open('classes.pkl','wb'))\n\n# create our training data\ntraining = []\n# create an empty array for our output\noutput_empty = [0] * len(classes)\n# training set, bag of words for each sentence\nfor doc in documents:\n    # initialize our bag of words\n    bag = []\n    # list of tokenized words for the pattern\n    pattern_words = doc[0]\n    # lemmatize each word - create base word, in attempt to represent related words\n    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n    # create our bag of words array with 1, if word match found in current pattern\n    for word in words:\n        bag.append(1) if word in pattern_words else bag.append(0)\n        \n    # output is a '0' for each tag and '1' for current tag (for each pattern)\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n    \n    training.append([bag, output_row])\n# shuffle our features and turn into np.array\nrandom.shuffle(training)\ntraining = np.array(training)\n# create train and test lists. X - patterns, Y - intents\ntrain_x = list(training[:,0])\ntrain_y = list(training[:,1])\nprint(\"Training data created\")\n\n# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n# equal to number of intents to predict output intent with softmax\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(train_y[0]), activation='softmax'))\n\n# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\nsgd = tf.keras.optimizers.legacy.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n#fitting and saving the model \nhist = model.fit(np.array(train_x), np.array(train_y), epochs=172, batch_size=5, verbose=1)\nmodel.save('chatbot_model.h5', hist)\n\nprint(\"model created\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:32.538331Z","iopub.execute_input":"2023-05-10T05:55:32.538844Z","iopub.status.idle":"2023-05-10T05:55:42.381558Z","shell.execute_reply.started":"2023-05-10T05:55:32.5388Z","shell.execute_reply":"2023-05-10T05:55:42.379983Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"api_key=\"5b28698846775264ccd4c0fb4b5599f9\"","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.383307Z","iopub.execute_input":"2023-05-10T05:55:42.383826Z","iopub.status.idle":"2023-05-10T05:55:42.390789Z","shell.execute_reply.started":"2023-05-10T05:55:42.383774Z","shell.execute_reply":"2023-05-10T05:55:42.388952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n@anvil.server.callable\ndef song_emotion():\n    # Create a SentimentIntensityAnalyzer object\n    analyzer = SentimentIntensityAnalyzer()\n\n    # Extract the last five elements of the msg list as the input text\n    len1 = len(msg)\n    text = msg[len1-1]+\" \"+msg[len1-2]+\" \"+msg[len1-3]+\" \"+msg[len1-4]+\" \"+msg[len1-5]\n\n    # Analyze the tone of the text\n    scores = analyzer.polarity_scores(text)\n    emotion = None\n\n    # Extract the emotion label from the response\n    if scores['compound'] >= 0.05:\n        emotion = 'positive'\n    elif scores['compound'] <= -0.05:\n        emotion = 'negative'\n    else:\n        emotion = 'neutral'\n    dic1 = dict()\n    dic1['emotion'] = emotion\n    import requests\n\n    url=f\"http://ws.audioscrobbler.com/2.0/?method=tag.gettoptracks&tag={emotion}&api_key={api_key}&format=json&limit=10\"\n    response = requests.get(url)\n    payload = response.json()\n    for i in range(10):\n        r=payload['tracks']['track'][i]\n        dic1[r['name']] = r['url']\n    return dic1","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.392883Z","iopub.execute_input":"2023-05-10T05:55:42.393412Z","iopub.status.idle":"2023-05-10T05:55:42.429666Z","shell.execute_reply.started":"2023-05-10T05:55:42.393354Z","shell.execute_reply":"2023-05-10T05:55:42.428629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\nurl=f\"http://ws.audioscrobbler.com/2.0/?method=tag.gettoptracks&tag=happy&api_key={api_key}&format=json&limit=5\"\nresponse = requests.get(url)\npayload = response.json()\n# for i in range(4):\nr=payload['tracks']['track'][0]\n# print(r['url'])\nprint(payload)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.430911Z","iopub.execute_input":"2023-05-10T05:55:42.431992Z","iopub.status.idle":"2023-05-10T05:55:42.570516Z","shell.execute_reply.started":"2023-05-10T05:55:42.43195Z","shell.execute_reply":"2023-05-10T05:55:42.569138Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"msg = list()\ntext = str()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.572109Z","iopub.execute_input":"2023-05-10T05:55:42.572517Z","iopub.status.idle":"2023-05-10T05:55:42.578831Z","shell.execute_reply.started":"2023-05-10T05:55:42.572479Z","shell.execute_reply":"2023-05-10T05:55:42.577223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@anvil.server.callable\ndef responsed(msg1):\n    msg.append(msg1)\n    ints = predict_class(msg1)\n    res = getResponse(ints, intents)\n    return res","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.580475Z","iopub.execute_input":"2023-05-10T05:55:42.580869Z","iopub.status.idle":"2023-05-10T05:55:42.592596Z","shell.execute_reply.started":"2023-05-10T05:55:42.580823Z","shell.execute_reply":"2023-05-10T05:55:42.591252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def my_bot():\n    print(\"Chatbot : Hey there, Wassup ?\")\n# responded function takes text of user and returns chatbot output\n    for i in range(5):\n        m = input(\"User : \")\n        res = responsed(m)\n        print(\"Chatbot : \"+res)\n    ans = song_emotion()\n    print(\"i'm analysing your mood from our conversation.....\")\n    print(\"Emotion : \"+ans['emotion'])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.594724Z","iopub.execute_input":"2023-05-10T05:55:42.59511Z","iopub.status.idle":"2023-05-10T05:55:42.606662Z","shell.execute_reply.started":"2023-05-10T05:55:42.595063Z","shell.execute_reply":"2023-05-10T05:55:42.604048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def my_response():\n    ans = song_emotion()\n    print(\"Emotion : \"+ans['emotion'])\n    ans.pop('emotion')\n    lst = list(ans.keys())\n    print(\"MY RECOMMENDATIONS TO YOU: \")\n    print(\"------------------------------------\")\n    for i in range(10):\n        print(\"Song_name : \"+lst[i])\n        print(\"Song_URL : \"+ans[lst[i]])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.608983Z","iopub.execute_input":"2023-05-10T05:55:42.609431Z","iopub.status.idle":"2023-05-10T05:55:42.620995Z","shell.execute_reply.started":"2023-05-10T05:55:42.609389Z","shell.execute_reply":"2023-05-10T05:55:42.619476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_bot()\nprint(\"---------------------------------------------\")\nprint(\"Recommendations\")\nprint(\"--------------------------------\n      -------------\")\nmy_response()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:55:42.622691Z","iopub.execute_input":"2023-05-10T05:55:42.623368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}